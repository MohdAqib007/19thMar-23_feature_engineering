{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5ca90-c519-49a8-8f2b-048db3e3d7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "#Ans-\n",
    "\n",
    "'''Min-Max scaling, also known as normalization, is a popular technique used in data preprocessing to rescale numerical features to a specific range. \n",
    "It transforms the values of a feature to a range between 0 and 1 or any other desired range.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "X(new) = [X - X(min)/ X(max) - X(min)]\n",
    "\n",
    "Where:\n",
    "X is the original value of a data point\n",
    "X(min) is the minimum value of the feature in the dataset\n",
    "X(max) is the maximum value of the feature in the dataset\n",
    "X(new) is the scaled value of the data point\n",
    "By using Min-Max scaling, all the values in the feature are linearly transformed to the range [0, 1]. \n",
    "This technique is particularly useful when the features have different scales or units, and we want to bring them to a common scale.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Let's say we have a dataset containing the ages of individuals ranging from 20 to 60. The original values of the ages are as follows:\n",
    "\n",
    "Ages =[20,25,30,35,40,45,50,55,60]\n",
    "\n",
    "To apply Min-Max scaling, we first find the minimum and maximum values of the ages. In this case, the minimum is 20 and the maximum is 60. \n",
    "Using the formula, we can scale each age to the range [0, 1]:\n",
    "\n",
    "Ages(scaled) = [(20-20)/(60-20), (25-20)/(60-20), (30-20)/(60-20), ....., (60-20)/(60-20)]\n",
    "\n",
    "Simplifying the equation, we get:\n",
    "Ages(scaled) =[0.0,0.083,0.167,0.25,0.333,0.417,0.5,0.583,0.667]\n",
    "Now, all the ages are scaled between 0 and 1, allowing for easier comparison and analysis.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19c1352-4734-4071-80de-280d6b44401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "#Ans-\n",
    "\n",
    "'''The Unit Vector technique, also known as normalization or feature scaling, is a method used to rescale numerical features such that each data point lies on a unit vector or has a magnitude of 1. \n",
    "Unlike Min-Max scaling, which brings the values to a specific range, Unit Vector scaling focuses on the direction or orientation of the data points in the feature space.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\n",
    "X(new) = X / ∥X∥\n",
    "\n",
    "Where:\n",
    "X is the original value of a data point \n",
    "∥X∥ represents the magnitude or Euclidean norm of X\n",
    "X(new) is the scaled value of the data point\n",
    "\n",
    "By dividing each data point by the magnitude of the feature vector, we ensure that the resulting vector has a magnitude of 1. \n",
    "This scaling technique is particularly useful in scenarios where the direction of the data points is more important than their specific values.\n",
    "\n",
    "Here's an example to illustrate the application of Unit Vector scaling:\n",
    "\n",
    "Let's consider a dataset that represents the height and weight of individuals. The original values of height and weight are as follows:\n",
    "\n",
    "Height=[150,170,160,180]\n",
    "Weight=[50,70,60,80]\n",
    "\n",
    "To apply Unit Vector scaling, we need to calculate the magnitude or Euclidean norm of each data point. The magnitude is calculated as:\n",
    "\n",
    "∥X∥ = ((X1)^2 + (X2)^2)**2\n",
    "\n",
    "Using this formula, we can calculate the magnitudes for each data point:\n",
    "\n",
    "∥Height∥ = ((150^2)+(170^2)+(106^2)+(180^2))**2 ≈ 365.57\n",
    "∥Weight∥= ((50^2)+(70^2)+(60^2)+(80^2))**2 ≈134.54\n",
    "\n",
    "Now, we can scale each data point by dividing it by its corresponding magnitude:\n",
    "\n",
    "Height(scaled) = [150/365.57, 170/365.57, 160/365.57, 180/365.57]\n",
    "Weight(scaled) = [50/134.54, 70/134.54, 60/134.54, 80/134.54]\n",
    "\n",
    "Simplifying the equations, we get:\n",
    "\n",
    "Height(scaled) ≈ [0.41, 0.46, 0.44,0.49]\n",
    "Weight(scaled) ≈ [0.37,0.52,0.45,0.60]\n",
    "\n",
    "Now, the data points for height and weight are scaled to unit vectors, allowing us to focus on the direction or orientation of the data rather than the specific values.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cc132-0dcb-48eb-9da8-507c8b8f0624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide anexample to illustrate its application.\n",
    "#Ans-\n",
    "\n",
    "'''PCA (Principal Component Analysis) is a widely used technique in dimensionality reduction. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important information or patterns present in the data. \n",
    "PCA achieves this by identifying the principal components, which are linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Here's a step-by-step overview of how PCA works:\n",
    "\n",
    "1. Standardize the data: Before applying PCA, it is common practice to standardize the dataset by subtracting the mean and dividing by the standard deviation of each feature. This step ensures that all features are on the same scale.\n",
    "\n",
    "2. Compute the covariance matrix: The covariance matrix is calculated based on the standardized data. It represents the relationships between different features and measures how they vary together. The covariance matrix provides insights into the linear dependencies between features.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues: The eigenvectors and eigenvalues are obtained from the covariance matrix. The eigenvectors represent the directions or principal components in the feature space, while the eigenvalues represent the amount of variance explained by each principal component. \n",
    "The eigenvectors with the highest eigenvalues capture the most significant variance in the data.\n",
    "\n",
    "4. Select the desired number of principal components: Depending on the desired dimensionality reduction, you can choose a subset of the eigenvectors (principal components) that correspond to the highest eigenvalues. \n",
    "These principal components collectively represent the most important patterns in the data.\n",
    "\n",
    "5. Transform the data: Finally, the original data is projected onto the selected principal components to obtain the lower-dimensional representation. This is achieved by taking the dot product between the standardized data and the chosen eigenvectors.\n",
    "\n",
    "Here's an example to illustrate the application of PCA:\n",
    "\n",
    "Consider a dataset with three features: height, weight, and age. The goal is to reduce the dimensionality of the dataset from three to two using PCA.\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "Height Weight Age\n",
    "170    60    30\n",
    "160    55    25\n",
    "175    65    35\n",
    "180    70    40\n",
    "\n",
    "1. Standardize the data: Subtract the mean and divide by the standard deviation of each feature.\n",
    "\n",
    "2. Compute the covariance matrix:\n",
    "\n",
    "            Height    Weight    Age\n",
    "Height      50.00     33.75     50.00\n",
    "Weight      33.75     25.00     33.75\n",
    "Age         50.00     33.75     50.00\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues:\n",
    "\n",
    "Eigenvalues: 108.19, 16.25, 0.56\n",
    "Eigenvectors:\n",
    "[ 0.63,  0.62, -0.47]\n",
    "[-0.57,  0.60, -0.57]\n",
    "[ 0.53, -0.53, -0.66]\n",
    "\n",
    "4. Select the principal components: In this case, we select the top two eigenvectors with the highest eigenvalues.\n",
    "\n",
    "5. Transform the data: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Resulting dataset (after dimensionality reduction):\n",
    "\n",
    "Principal Component 1  Principal Component 2\n",
    "0.39                  0.41\n",
    "-0.35                 0.31\n",
    "0.40                  -0.42\n",
    "0.55                  -0.41\n",
    "\n",
    "The original three-dimensional dataset is transformed into a two-dimensional representation using PCA. \n",
    "This lower-dimensional representation retains the most important information or patterns present in the original data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f3959a-a17e-4b43-aedd-f7c36e56947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "#Ans-\n",
    "\n",
    "'''PCA and feature extraction are closely related concepts. PCA can be used as a feature extraction technique to transform a high-dimensional dataset into a lower-dimensional representation, where the new features (principal components) are linear combinations of the original features.\n",
    "\n",
    "Feature extraction aims to create a new set of features that can represent the data effectively while reducing the dimensionality. By using PCA for feature extraction, we can identify the most important patterns or information in the data and create a reduced set of features that capture most of the variance.\n",
    "\n",
    "Here's an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "Consider a dataset with five features: height, weight, age, income, and education level. The goal is to extract a lower-dimensional representation of the data using PCA.\n",
    "\n",
    "Original dataset:\n",
    "\n",
    "Height  Weight  Age  Income  Education Level\n",
    "170     60      30   50000   12\n",
    "160     55      25   45000   10\n",
    "175     65      35   60000   14\n",
    "180     70      40   70000   16\n",
    "\n",
    "1. Standardize the data: Subtract the mean and divide by the standard deviation of each feature.\n",
    "\n",
    "2. Compute the covariance matrix.\n",
    "\n",
    "3. Compute the eigenvectors and eigenvalues.\n",
    "\n",
    "4. Select the desired number of principal components.\n",
    "\n",
    "5. Suppose we decide to select the top two principal components with the highest eigenvalues.\n",
    "\n",
    "Transform the data: Project the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n",
    "Resulting dataset (after feature extraction):\n",
    "\n",
    "Principal Component 1  Principal Component 2\n",
    "-0.26                 -0.13\n",
    "-0.07                 -0.25\n",
    "0.37                  0.04\n",
    "0.57                  0.34\n",
    "In this example, PCA is used for feature extraction to reduce the five original features to two principal components. The resulting dataset captures the most important patterns or variations in the original data, while significantly reducing the dimensionality.\n",
    "\n",
    "The new features obtained from PCA can be used for subsequent analysis, such as clustering or classification, or to visualize the data in a lower-dimensional space. \n",
    "Feature extraction using PCA helps to simplify the data representation, eliminate redundant or less informative features, and potentially improve the efficiency and performance of downstream machine learning algorithms.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf16e7b-81c3-498e-ba11-920fc330bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "#Ans-\n",
    "\n",
    "'''To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling to normalize the numerical features such as price, rating, and delivery time. Here's a step-by-step approach:\n",
    "\n",
    "1. Understand the dataset: Familiarize yourself with the dataset and the specific features you want to preprocess, such as price, rating, and delivery time. Determine their current range and distribution.\n",
    "\n",
    "2. Identify the minimum and maximum values: Find the minimum and maximum values for each feature in the dataset. For example, for the price feature, find the minimum and maximum prices among all the food items.\n",
    "\n",
    "3. Apply Min-Max scaling: Use the Min-Max scaling formula to transform each feature to the desired range (usually between 0 and 1):\n",
    "X(new) = [X - X(min)/ X(max) - X(min)]\n",
    "\n",
    "Here, X represents the original value of a data point, X(min) is the minimum value of the feature, X(max) is the maximum value of the feature, and X(new) is the scaled value of the data point.\n",
    "Apply this scaling formula to each feature you want to preprocess. Repeat the process for every data point in the dataset.\n",
    "\n",
    "4. Update the dataset: Replace the original values of the features with their corresponding scaled values obtained from the Min-Max scaling process. Update the dataset accordingly.\n",
    "\n",
    "By using Min-Max scaling, you bring all the numerical features to the same scale, which can be crucial for building a recommendation system. It ensures that no single feature dominates the others due to its larger range or magnitude. \n",
    "This preprocessing step allows for fair comparisons and helps capture relative differences among the features accurately.\n",
    "\n",
    "After applying Min-Max scaling, the dataset will have all the numerical features (e.g., price, rating, delivery time) scaled to the range of 0 to 1, allowing for more effective analysis, modeling, and recommendation generation in the food delivery recommendation system.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19332d21-4c6e-4f44-a40c-72b41817e5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "#Ans-\n",
    "\n",
    "'''To reduce the dimensionality of a dataset containing many features for building a model to predict stock prices, PCA (Principal Component Analysis) can be applied. \n",
    "Here's a step-by-step approach on how you can use PCA for dimensionality reduction in the context of predicting stock prices:\n",
    "\n",
    "1. Understand the dataset: Familiarize yourself with the dataset and the available features related to company financial data and market trends. Identify the numerical features that are relevant to stock price prediction.\n",
    "\n",
    "2. Data preprocessing: Perform any necessary data preprocessing steps, such as handling missing values, handling categorical variables, and standardizing the numerical features. PCA is sensitive to the scale of the features, so it's crucial to standardize the data before applying PCA.\n",
    "\n",
    "3. Apply PCA: Apply PCA to the standardized numerical features. The goal is to identify the principal components that capture the most significant variance in the dataset.\n",
    "\n",
    "4. Determine the number of principal components: Analyze the explained variance ratio associated with each principal component. The explained variance ratio indicates the proportion of variance explained by each principal component. \n",
    "Choose the number of principal components that collectively explain a significant amount of variance in the data. This selection can be based on a cumulative explained variance threshold or a specific number of components that capture a certain percentage of variance.\n",
    "\n",
    "5. Transform the data: Transform the original dataset by projecting it onto the selected principal components. This transformation will create a lower-dimensional representation of the dataset, where the new features are the principal components.\n",
    "\n",
    "6. Feature selection: Assess the importance of the original features in the reduced-dimensional space. PCA provides a measure called the loading or contribution of each original feature to each principal component. \n",
    "If certain original features have high loadings on a few principal components, it indicates that those features carry crucial information and should be considered for further analysis.\n",
    "\n",
    "7. Model training: Use the reduced-dimensional dataset, containing the selected principal components, to train your stock price prediction model. You can apply various machine learning algorithms or statistical models to build the prediction model.\n",
    "\n",
    "By applying PCA, you can effectively reduce the dimensionality of the dataset while retaining the most important patterns or variations in the data. \n",
    "This can help in avoiding the curse of dimensionality, improving computational efficiency, and potentially enhancing the model's ability to generalize and make accurate predictions of stock prices.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2ce08ad-3d22-4cce-bbd1-c24e99492a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "#Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "#Ans-\n",
    "\n",
    "'''Min-Max scaling, also known as normalization, is a popular technique used in data preprocessing to rescale numerical features to a specific range. \n",
    "It transforms the values of a feature to a range between 0 and 1 or any other desired range.\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\n",
    "X(new) = [X - X(min)/ X(max) - X(min)]\n",
    "\n",
    "Where:\n",
    "X is the original value of a data point\n",
    "X(min) is the minimum value of the feature in the dataset\n",
    "X(max) is the maximum value of the feature in the dataset\n",
    "X(new) is the scaled value of the data point\n",
    "By using Min-Max scaling, all the values in the feature are linearly transformed to the range [0, 1]. \n",
    "This technique is particularly useful when the features have different scales or units, and we want to bring them to a common scale.\n",
    "\n",
    "\n",
    "To perform Min-Max scaling on the dataset [1, 5, 10, 15, 20] and transform the values to a range of -1 to 1, follow these steps:\n",
    "\n",
    "1. Find the minimum and maximum values in the dataset:\n",
    "\n",
    "Minimum value (X_min) = 1\n",
    "Maximum value (X_max) = 20\n",
    "\n",
    "2. Apply the Min-Max scaling formula:\n",
    "\n",
    "X(new) = [X - X(min)/ X(max) - X(min)] × (max(new) − min(new)) +min(new)\n",
    "\n",
    "In this case, since we want to transform the values to a range of -1 to 1, we have:\n",
    "\n",
    "Minimum value of the new range (min_{\\text{new}}) = -1\n",
    "Maximum value of the new range (max_{\\text{new}}) = 1\n",
    "Plugging in the values, the Min-Max scaling formula becomes:\n",
    "\n",
    "X(new) = [(20−1)/(X−1)]×(1−(−1))+(−1)\n",
    "\n",
    "Calculate the scaled values for each data point:\n",
    "\n",
    "For X = 1:\n",
    "\n",
    "X(new) = (20−1)/(1−1)×(1−(−1))+(−1) = −1\n",
    "\n",
    "Similarly after calculation for  5, 10, 15, 20 values of X(new) are -0.57894737, -0.05263158,  0.47368421,  1 respectively.\n",
    "\n",
    "The Min-Max scaled values for the dataset [1, 5, 10, 15, 20] in the range of -1 to 1 are:\n",
    "[-1, -0.57894737, -0.05263158,  0.47368421,  1]\n",
    "'''\n",
    "\n",
    "\n",
    "#Python code for same\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new range\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Find the minimum and maximum values\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "\n",
    "# Apply Min-Max scaling formula\n",
    "scaled_data = ((data - data_min) / (data_max - data_min)) * (new_max - new_min) + new_min\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be470ef-7e15-4e65-888d-216185d82fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "#Ans-\n",
    "'''To determine the number of principal components to retain in a dataset containing features like height, weight, age, gender, and blood pressure, we would follow these steps:\n",
    "\n",
    "1. Preprocess the data: Before applying PCA, it is essential to preprocess the data. This includes handling missing values, encoding categorical variables (like gender), and standardizing numerical features (like height, weight, age, and blood pressure). \n",
    "Standardization ensures that all the features have similar scales, which is a requirement for PCA.\n",
    "\n",
    "2. Apply PCA: Once the data is preprocessed, we can apply PCA to extract the principal components. PCA calculates the eigenvectors (principal components) that capture the maximum variance in the dataset.\n",
    "\n",
    "3. Evaluate explained variance ratio: After applying PCA, we examine the explained variance ratio associated with each principal component. The explained variance ratio indicates the proportion of variance explained by each principal component.\n",
    "\n",
    "D4. etermine the number of principal components: We need to decide how many principal components to retain based on the explained variance ratio. A common approach is to set a threshold for the cumulative explained variance, such as retaining principal components that collectively explain a certain percentage (e.g., 90%) of the total variance. \n",
    "Alternatively, we can specify the desired number of principal components based on the context and desired level of dimensionality reduction.\n",
    "\n",
    "The choice of how many principal components to retain depends on the specific dataset and the goals of the analysis. Generally, we aim to retain a sufficient number of principal components that capture most of the variance while reducing the dimensionality of the data.\n",
    "\n",
    "To make an informed decision, we would examine the cumulative explained variance ratio and choose the number of principal components that collectively explain a high percentage (e.g., 90% or more) of the total variance. This ensures that we retain the majority of the information in the dataset while reducing its dimensionality.\n",
    "\n",
    "In the case of the dataset with features like height, weight, age, gender, and blood pressure, it is difficult to determine the number of principal components without the actual data and its characteristics. \n",
    "However, a typical approach would be to analyze the cumulative explained variance and select the smallest number of principal components that explain a substantial portion (e.g., 90% or more) of the total variance.\n",
    "\n",
    "It's worth noting that the specific application and requirements may influence the choice of the number of principal components to retain. It's always a trade-off between reducing dimensionality and retaining enough information to achieve the desired analytical goals.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91654372-6e69-4759-aeef-928c59cce4ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04310d65-d071-443c-8181-b1d59b2e0d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
